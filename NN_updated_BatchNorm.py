import numpy as np
import math
from keras.datasets import mnist
import time
import pickle

epsilon = 1e-7

def initialize_parameters(layer_dims):
    """
    input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)

    output: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).

    Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively
    """
    output = {}
    input_layer_dim = layer_dims[0]
    for layer_id in range(1, len(layer_dims)):
        output_layer_dim = layer_dims[layer_id]
        weights = np.random.randn(output_layer_dim, input_layer_dim)*np.sqrt(2/input_layer_dim)
        bias = np.zeros(output_layer_dim).reshape(-1, 1)
        output[layer_id] = (weights, bias)
        input_layer_dim = output_layer_dim
    return output


def linear_forward(A, W, b):
    """
    Description: Implement the linear part of a layer's forward propagation.

    input:
    A – the activations of the previous layer
    W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    B – the bias vector of the current layer (of shape [size of current layer, 1])

    Output:
    Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    linear_cache = {'W':W, 'A':A, 'b':b}
    Z = np.matmul(W, A)+b
    return Z, linear_cache


def softmax(Z):
    """
    Input:
    Z – the linear component of the activation function

    Output:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation

    note:
    Softmax can be thought of as a sigmoid for multi-class problems. The formula for softmax for each node in the output layer is as follows:
    Softmax〖(z)〗_i=(exp⁡(z_i))/(∑_j▒〖exp⁡(z_j)〗)
    """
    exp_Z = np.exp(Z-np.max(Z, axis=0))
    A = exp_Z/exp_Z.sum(axis=0)
    return A, Z


def relu(Z):
    """
    Input:
    Z – the linear component of the activation function

    Output:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    """
    A=Z
    A[A <= 0] = 0
    return A, Z


def linear_activation_forward(A_prev, W, B, activation):
    """
    Description:
    Implement the forward propagation for the LINEAR->ACTIVATION layer

    Input:
    A_prev – activations of the previous layer
    W – the weights matrix of the current layer
    B – the bias vector of the current layer
    Activation – the activation function to be used (a string, either “softmax” or “relu”)

    Output:
    A – the activations of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    A = 0
    if activation == 'softmax':
        A, activation_cache = softmax(Z)
    if activation == 'relu':
        A, activation_cache = relu(Z)

    cache = {"linear_cache": linear_cache, "activation_cache": activation_cache}
    return A, cache


def L_model_forward(X, parameters, use_batchnorm):
    """
    Description:
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

    Input:
    X – the data, numpy array of shape (input size, number of examples)
    parameters – the initialized W and b parameters of each layer
    use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).

    Output:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """
    A_prev = X
    caches = []
    for layer_index, hidden_layer in parameters.items():
        if not isinstance(layer_index, str):
            hidden_layer_weights, hidden_layer_bias = hidden_layer
            activation = 'relu'
            if layer_index == len(parameters)-4:
                # final layer- softmax
                activation = 'softmax'
            A, linear_cache = linear_activation_forward(A_prev, hidden_layer_weights, hidden_layer_bias, activation)
            if use_batchnorm and layer_index != len(parameters)-4:
                # calc mean and standard deviation
                if parameters['mode'] == 'train':
                    # save batchnorm mean and std
                    mean = np.mean(A, axis=1).reshape(-1, 1)
                    std = np.std(A, axis=1).reshape(-1, 1)
                    parameters['batchNorm_mean'][layer_index].append(mean)
                    parameters['batchNorm_std'][layer_index].append(std)
                    A = apply_batchnorm(A)
                else:
                    # in test mode - use mean and std learnt in training
                    mean = np.mean(parameters['batchNorm_mean'][layer_index][max(0, len(parameters['batchNorm_mean'][layer_index])-100):], axis=0)
                    std = np.mean(parameters['batchNorm_std'][layer_index][max(0, len(parameters['batchNorm_std'][layer_index])-100):], axis=0)
                    A = A - mean
                    A /= np.sqrt(std + epsilon)
            A_prev = A
            caches.append(linear_cache)
    return A, caches


def	compute_cost(AL, Y):
  """
  Description:
  Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss. The formula is as follows :
  cost=-1/m*∑_1^m▒∑_1^C▒〖y_i  log⁡〖(y ̂)〗 〗, where y_i is one for the true class (“ground-truth”) and y ̂ is the softmax-adjusted prediction (this link provides a good overview).

  Input:
  AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
  Y – the labels vector (i.e. the ground truth) one hot matrix, shape (num_of_classes, number of examples)

  Output:
  cost – the cross-entropy cost
  """
  number_of_examples = AL.shape[1]
  AL_log = -np.log(AL + epsilon)
  target_prob = np.sum(np.multiply(Y, AL_log),axis=0)
  cost = np.sum(target_prob)
  cost/=number_of_examples
  return cost

def apply_batchnorm(A):
  """
  Description:
  performs batchnorm on the received activation values of a given layer.

  Input:
  A - the activation values of a given layer

  output:
  NA - the normalized activation values, based on the formula learned in class
  """
  mean = np.mean(A, axis=1).reshape(-1,1)
  std = np.std(A, axis=1).reshape(-1,1)
  NA = A-mean
  NA /= np.sqrt(std + epsilon)
  return NA


def Linear_backward(dZ, cache):
    """
    Description:
    Implements the linear part of the backward propagation process for a single layer

    Input:
    dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
    cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Output:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev = cache['A']
    W = cache['W']
    m = A_prev.shape[1]

    dW = (1 / m) * np.matmul(dZ, A_prev.T)
    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.matmul(W.T, dZ)
    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Description:
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.

    Input:
    dA – post activation gradient of the current layer
    cache – contains both the linear cache and the activations cache
    activation - the activation function ( either “softmax” or “relu”).

    Output:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW – Gradient of the cost with respect to W (current layer l), same shape as W
    db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    if activation == "softmax":
        dZ = softmax_backward(dA, cache["activation_cache"])
    else:
        dZ = relu_backward(dA, cache["activation_cache"])

    dA_prev, dW, db = Linear_backward(dZ, cache["linear_cache"])
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a ReLU unit

    Input:
    dA – the post-activation gradient
    activation_cache – contains Z (stored during the forward propagation)

    Output:
    dZ – gradient of the cost with respect to Z
    """
    activation_cache[activation_cache <= 0] = 0
    activation_cache[activation_cache > 0] = 1
    dZ = np.multiply(dA, activation_cache)
    return dZ


def softmax_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a softmax unit

    Input:
    dA – the post-activation gradient
    activation_cache – contains Z (stored during the forward propagation)

    Output:
    dZ – gradient of the cost with respect to Z
    """
    A, activation_cache = softmax(activation_cache)
    dZ = A - dA
    return dZ


def L_model_backward(AL, Y, caches):
    """
    Description:
    Implement the backward propagation process for the entire network.

    Some comments:
    the backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done iteratively over all the remaining layers of the network.

    Input:
    AL - the probabilities vector, the output of the forward propagation (L_model_forward)
    Y - the true labels vector (the "ground truth" - true classifications)
    Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

    Output:
    Grads - a dictionary with the gradients
                grads["dA" + str(l)] = ...
                grads["dW" + str(l)] = ...
                grads["db" + str(l)] = ...
    """
    grads = {}
    layer_id = len(caches)
    cc = caches[layer_id - 1]
    dA_prev, dW, db = linear_activation_backward(Y, cc, 'softmax')

    grads["dA" + str(layer_id)] = dA_prev
    grads["dW" + str(layer_id)] = dW
    grads["db" + str(layer_id)] = db
    layer_id = layer_id - 1
    while layer_id > 0:
        cc = caches[layer_id - 1]
        dA_prev, dW, db = linear_activation_backward(dA_prev, cc, 'relu')
        grads["dA" + str(layer_id)] = dA_prev
        grads["dW" + str(layer_id)] = dW
        grads["db" + str(layer_id)] = db
        layer_id = layer_id - 1
    return grads


def Update_parameters(parameters, grads, learning_rate):
    """
    Description:
    Updates parameters using gradient descent

    Input:
    parameters – a python dictionary containing the DNN architecture’s parameters
    grads – a python dictionary containing the gradients (generated by L_model_backward)
    learning_rate – the learning rate used to update the parameters (the “alpha”)

    Output:
    parameters – the updated values of the parameters object provided as input
    """
    for layer_id in parameters:
        if not isinstance(layer_id, str):
            grad_W_key = "dW" + str(layer_id)
            grad_b_key = "db" + str(layer_id)
            weights = parameters[layer_id][0] - learning_rate * grads[grad_W_key]
            bias = parameters[layer_id][1] - learning_rate * grads[grad_b_key]
            parameters[layer_id] = (weights, bias)
    return parameters


def generateBatches(X, Y, batch_size):
    """
    Description:
    create batches from entire data.

    Input:
    X - input data
    Y - the ground truth
    batch_size - the number of examples in a single training batch

    Output:
    batchesX : list of batches of the input data.
    batchesY : list of batches of the labels.
    """
    dev = math.ceil(X.shape[0] / batch_size)
    batchesX = []
    batchesY = []

    for i in range(dev):
        batchesX.append(X[i * batch_size:(i + 1) * batch_size].T)
        batchesY.append(Y[i * batch_size:(i + 1) * batch_size].T)

    return batchesX, batchesY


def train_val_split(X, Y, split_size):
    """
    Description: split the training set to train set and validation set

    Input:
    X: input data
    Y: the ground truth
    split_size: percentage of validation to extract from input data
    Output:
    train_X: train data
    val_X: validation data
    train_Y: train ground truth
    val_Y: validation ground truth
    """
    X, Y = X.T, Y.T
    val_size =math.ceil(X.shape[0]*split_size)
    indices = np.random.permutation(X.shape[0])
    training_idx, test_idx = indices[:X.shape[0]-val_size], indices[X.shape[0]-val_size:]
    train_X, val_X = X[training_idx, :], X[test_idx, :]
    train_Y, val_Y = Y[training_idx, :], Y[test_idx, :]

    return train_X.T, val_X.T, train_Y.T, val_Y.T


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):
    """
    Description:
    Implements a L-layer neural network. All layers but the last should have the ReLU activation function, and the final layer will apply the softmax activation function.
    The size of the output layer should be equal to the number of labels in the data.
    Please select a batch size that enables your code to run well (i.e. no memory overflows while still running relatively fast).

    Hint: the function should use the earlier functions in the following order: initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters

    Input:
    X – the input data, a numpy array of shape (height*width , number_of_examples)
    Comment: since the input is in grayscale we only have height and width, otherwise it would have been height*width*3
    Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    Layer_dims – a list containing the dimensions of each layer, including the input
    batch_size – the number of examples in a single training batch.

    Output:
    parameters – the parameters learnt by the system during the training (the same parameters that were updated in the update_parameters function).
    costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).
    """
    batchnorm = True
    parameters = initialize_parameters(layers_dims)
    costs = {}
    training_steps_improvement= []
    # split the original train set to train set and validation set
    # Not allowed to use train_test_split
    train_X, val_X, train_Y, val_Y = train_val_split(X, Y, 0.2)

    batchesX, batchesY = generateBatches(train_X.T, train_Y.T, batch_size)
    # save batchnorm mean and std in parameters
    parameters['batchNorm_mean']={}
    parameters['batchNorm_std']={}
    for i in range(1, len(layers_dims)):
        parameters['batchNorm_mean'][i]=[]
        parameters['batchNorm_std'][i]=[]

    parameters['batchnorm'] = batchnorm
    batch_id = 0
    epoch_number = 1
    for iter_id in range(1,num_iterations):
        tempY = batchesY[batch_id]
        tempX = batchesX[batch_id]
        batch_id = (batch_id+1)%(len(batchesX))
        if batch_id==0:
            epoch_number+=1
        parameters['mode'] = 'train'
        z, caches = L_model_forward(tempX, parameters, batchnorm)
        grads = L_model_backward(z, tempY, caches)
        parameters = Update_parameters(parameters, grads, learning_rate)

        if iter_id % 100 == 0:
            cost = compute_cost(z, tempY)
            costs[iter_id]=cost
            print(f'cost for iter:{iter_id}, {cost}')

        batch_eval = Predict(val_X, val_Y, parameters)
        training_steps_improvement.append(batch_eval)

        if len(training_steps_improvement)>200:
            previous_eval_mean = np.mean(
                training_steps_improvement[max(0, len(training_steps_improvement) - 200): len(training_steps_improvement)-100])
            current_eval_mean = np.mean(
                training_steps_improvement[
                max(0, len(training_steps_improvement) - 100): len(training_steps_improvement)])
            if abs(current_eval_mean-previous_eval_mean)<epsilon:
                print(f'model converges in iteration number {iter_id}, epoch number {epoch_number}, stop training')
                print(f'final model accuracy for the training set:{Predict(train_X, train_Y, parameters)}')
                print(f'final model accuracy for the validation set:{batch_eval}')
                break

    # save training_steps_improvement and costs into files
    # save_to_file(costs, training_steps_improvement)

    return parameters, costs


def Predict(X, Y, parameters):
  """
  Description:
  The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.

  Input:
  X – the input data, a numpy array of shape (height*width, number_of_examples)
  Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
  Parameters – a python dictionary containing the DNN architecture’s parameters

  Output:
  accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for which the correct label receives the hughest confidence score).
  Use the softmax function to normalize the output values.
  """
  parameters['mode'] = 'predict'
  y_tag, caches = L_model_forward(X, parameters, parameters['batchnorm'])
  y_tag = np.argmax(y_tag, axis=0)
  idy = np.argmax(Y, axis=0)
  accuracy = np.mean(y_tag == idy)
  return accuracy


def create_one_hot_vector_y(y):
    """
    Description: create one hot vector from ground truth labels

    Input:
    y: ground truth
    Output: one hot vector from ground truth labels
    """
    b = np.zeros((y.size, y.max() + 1))
    b[np.arange(y.size), y] = 1
    return b


def standard_scale(X):
    """
    Description: scale the input data by standard scale

    Input:
    X: input data
    Output: input data scaled
    """
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_scale = X - mean
    X_scale /= np.sqrt(std + epsilon)
    return X_scale


def load_mnist():
    """
    Description:
    1. load the mnist dataset using keras.datasets.
    2. flatten the input data to a matrix of [m,784]
    3. scale the train data

    Output: MNIST data scaled
    """
    (train_X, train_y), (test_X, test_y) = mnist.load_data()
    train_X = train_X.reshape((-1, 784))
    test_X = test_X.reshape((-1, 784))
    train_y = create_one_hot_vector_y(train_y)
    test_y = create_one_hot_vector_y(test_y)
    # scale input
    train_X = standard_scale(train_X)
    test_X = standard_scale(test_X)
    return train_X, test_X, test_y, train_y


def driver1():
    layers_dim = [2, 2, 2]
    params = initialize_parameters(layers_dim)
    print("initialize_parameters output:")
    print(params)

    input = np.random.randn(2, 3)
    print("input:")
    print(input)

    Z, linear_cache = linear_forward(input, params[1][0], params[1][1])
    print("linear_forward output:")
    print(Z)

    AL, cache = L_model_forward(input, params, False)
    Y = np.array([[0, 1], [1, 0], [1, 0]]).T
    compute_cost(AL, Y)

    grads = L_model_backward(AL, Y, cache)
    print("grads:")
    print(grads)

    params, costs = L_layer_model(input, Y, layers_dim, 0.05, 3000, 2, False)
    print("params:")
    print(params)
    print("costs:")
    for i in range(len(costs)):
        print("iteration number " + str(i * 100) + " cost: " + str(costs[i]))


def driver2():
    learning_rate = 0.009
    layers_dims = [784, 20, 7, 5, 10]
    num_iterations = 12000
    batch_size = 32

    X_train, X_test, y_test, y_train = load_mnist()

    start_time = time.time()
    params, costs = L_layer_model(X_train.T, y_train.T, layers_dims, learning_rate, num_iterations,
                                  batch_size)
    print(f'model is done training, training time: {(time.time() - start_time)}')
    test_accuracy = Predict(X_test.T, y_test.T, params)
    print(f'test_accuracy: {test_accuracy}')


def save_to_file(costs, validation_accuracy):
    """
    Description: save training statistics as costs and model accuracy on validation set

    Input:
    costs: dictionary with costs for each 100 training steps
    validation_accuracy: list with model accuracy on validation set for all training steps

    """
    with open('costs.pkl', 'wb') as output:
        pickle.dump(costs, output, pickle.HIGHEST_PROTOCOL)

    with open('validation_accuracy.pkl', 'wb') as output:
        pickle.dump(validation_accuracy, output, pickle.HIGHEST_PROTOCOL)


if __name__ == "__main__":
    driver2()
